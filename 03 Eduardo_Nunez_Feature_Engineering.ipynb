{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el presente Notebook se continuará con los datos limpios generados en la etapa anterior, y se prepararán para utilizarlos en el modelo de Machine Learning. También se elijirá cual es el modelo adecuado para nuestro caso de análisis.\n",
    "\n",
    "La etapa de análisis de datos y visualización se puede encontrar en el Notebook [*02 Eduardo_Nunez_Analisis_Datos.ipynb*](https://github.com/EduGatoX/datascience02/blob/main/02%20Eduardo_Nunez_Analisis_Datos.ipynb) alojado en GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos limpios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos alojados en [*videogame_data_clean.csv*](https://github.com/EduGatoX/datascience02/blob/main/videogame_data_clean.csv) ya han sido previamente limpiados, es decir, se imputaron nulos y se eliminaron outliers en la etapa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"videogame_data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminación de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se eliminan las columnas:\n",
    "\n",
    "- **Unnamed: 0**: Esta columna se elimina porque proviene del guardado anterior en donde no se indicó a la función que la primera columna era index.\n",
    "- **Name**: Se eliminará la variable *Name* ya que nos interesa evaluar el desempeño de los videojuegos desde un punto de vista general y no idiosincrático a un juego en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Unnamed: 0\", \"Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformación de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se desea evaluar un modelo para la variable target **Global_Sales**. De esta forma, las variables *NA_Sales*, *EU_Sales*, *JP_Sales* y *Other_Sales* parecen redudantes ya que **Global_Sales** es la suma de las anteriores. Sin embargo, se procede a transformar estas variables a variables porcentuales respecto de **Global Sales** de forma de que representen la distribución de ventas en esas localidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se recupera la variable de interes \"Global_Sales\"\n",
    "df[\"Global_Sales\"] = df[\"NA_Sales\"]+df[\"EU_Sales\"]+df[\"JP_Sales\"]+df[\"Other_Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se transforman las variables de ventas en forma porcentual\n",
    "df[[\"NA_Sales\", \"EU_Sales\", \"JP_Sales\", \"Other_Sales\"]] = df[[\"NA_Sales\", \"EU_Sales\", \"JP_Sales\", \"Other_Sales\"]].div(df[\"Global_Sales\"], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se realiza One Hot Encoding de las variables categoricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de variables numéricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminación por coeficiente de correlación\n",
    "\n",
    "Se realizan dos chequeos:\n",
    "- Baja correlación con *\"target\"*: Se evalúan coeficientes de correlación bajos respecto de la variable target **Global_sales**, se considera un bajo coeficiente de correlación a un valor menor a **0.2**. Sin embargo, las variables *NA_Sales*, *EU_Sales*, *JP_Sales* y *Other_Sales* no se tocarán ya que tienen importancia teórica en el negocio.\n",
    "\n",
    "- Multicolinealidad entre variables: Se eliminan las variables que tengan un coeficiente de correlación mayor a **0.9** entre sí.\n",
    "\n",
    "Se usará la medida de correlación de **Spearman** ya que puede detectar relaciones no-lineales entre las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df[[\"Year\", \"NA_Sales\", \"EU_Sales\", \"JP_Sales\", \"Other_Sales\", \"Global_Sales\",\n",
    "                 \"playtime\", \"metacritic\", \"rating\", \"exceptional\", \"recommended\",\n",
    "                 \"meh\", \"skip\"]]\n",
    "\n",
    "spearman_corr = df_numeric.corr(method=\"spearman\")\n",
    "\n",
    "high_corr = spearman_corr[(spearman_corr > 0.8) | (spearman_corr < -0.8)]\n",
    "low_corr = spearman_corr[(spearman_corr < 0.2) & (spearman_corr > -0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(high_corr,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            cbar=True,\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": 10},\n",
    "            cmap=\"coolwarm\",\n",
    "            ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(low_corr,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            cbar=True,\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": 10},\n",
    "            cmap=\"coolwarm\",\n",
    "            ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De lo anterior es posible concluir que:\n",
    "\n",
    "- Las variables *Year*, *playtime*, y *rating* tienen baja correlación con la variable **Global_Sales**. Por lo tanto, estas variables serán eliminadas\n",
    "- Las variables *exceptional*, *recommended*, *meh* y *skip* tienen una alta correlación entre sí. Por lo tanto, existe una relación entre ellas y se realizará reducción de dimensionalidad entre esas variables utilizando el método PCA (Principal Component Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"Year\", \"playtime\", \"rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reducción de dimensionalidad (Método PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Del punto anterior se observó que las variables *exceptional*, *recommended*, *meh* y *skip* tienen una alta correlación entre sí. Esto sugiere que existe multicolinealidad entre dichas variables. Por lo tanto, se utilizará el método PCA para detectar el parámetro principal que las define."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se identifican las variables a transformar y se realiza el escalado de los datos para ejecutar PCA ya que este método es sensible a la escala de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se define la variable X como las columnas a transformar\n",
    "X = df[[\"exceptional\", \"recommended\", \"meh\", \"skip\"]].copy(deep=True)\n",
    "\n",
    "# PCA es sensitivo a la escala por lo que se debe estandarizar\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ejecuta el método PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None) # se mantienen todos los componentes\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evalúa la varianza explicada por las variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se mantienen las variables que explican una varianza acumulada de un **95%**. Se detecta que las dos primeras variables del PCA son capaces de explicar el 95% de la varianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Varianza acumulada\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "print(\"Varianza acumulada explicada:\", cumulative_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(\"Numero de componentes necesarios para tener una varianza > 95%:\", n_components_95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se reduce la dimensionalidad utilizando la cantidad de componentes necesarios para mantener una varianza explicada superior a un 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components_95)\n",
    "X_reduced = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se agregan los componentes al Dataframe original y se eliminan las variables originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(X_reduced, columns=[f\"PC{i+1}\" for i in range(n_components_95)])\n",
    "df = pd.concat([df, df_pca], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"exceptional\", \"recommended\", \"meh\", \"skip\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se evalúa la contribución de componentes originales en la componentes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Contribución de componentes:\\n\",pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se vuelve a evaluar la matriz de correlacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric = df[[\"NA_Sales\", \"EU_Sales\", \"JP_Sales\", \"Other_Sales\", \"Global_Sales\",\n",
    "                \"metacritic\", \"PC1\", \"PC2\"]]\n",
    "spearman_corr = df_numeric.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(spearman_corr,\n",
    "            vmin=-1,\n",
    "            vmax=1,\n",
    "            cbar=True,\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            fmt=\".2f\",\n",
    "            annot_kws={\"size\": 10},\n",
    "            cmap=\"coolwarm\",\n",
    "            ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, puede observarse que la variable PC2 tiene una baja correlación con **Global_Sales**, por lo tanto, se eliminará del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"PC2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de variables categóricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para seleccionar las variables categóricas se utiliza el test chi-cuadrado para evaluar la independencia entre las variables categóricas y las variable **Global_Sales**. Para realizar esto se realizara un test F ANOVA, utilizando la función de scikit-learn `SelectKBest` con `f_regression` como *scoring function*.\n",
    "\n",
    "Se evalúan los *f-scores* y los *p-values* para cada categoría y se mantienen aquellas categoría cuyos *p-values* sean menores a **0.05**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se extraen los nombres de las columnas categóricas\n",
    "numerical_columns = [\"NA_Sales\", \"EU_Sales\", \"JP_Sales\", \"Other_Sales\", \"metacritic\", \"PC1\"]\n",
    "categorical_columns = list(set(df.columns) - set(numerical_columns))\n",
    "len(categorical_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[categorical_columns]\n",
    "y = df[\"Global_Sales\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "# Realizar fit para evaluar el score usando la funcion f_regression\n",
    "anova_selector = SelectKBest(score_func=f_regression, k=\"all\")\n",
    "anova_scores = anova_selector.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_scores, p_values = f_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"ANOVA_Score\": anova_scores.scores_\n",
    "}).sort_values(by=\"ANOVA_Score\", ascending=False)\n",
    "feature_scores[\"F_Score\"] = f_scores\n",
    "feature_scores[\"p_values\"] = p_values\n",
    "feature_scores = feature_scores[feature_scores[\"p_values\"] >= 0.05]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se mantendrán sólo las categorías cuyo *p-value* sea menor a **0.05**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns = feature_scores[\"Feature\"].values\n",
    "df = df.drop(columns=drop_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guardado del dataset posterior a la selección de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se almacena el dataset obtenido para modelado de machine learning posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"videogame_data_model.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".data-science-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
